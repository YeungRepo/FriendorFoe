{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=np.load('Thousand_bp_trial2.npz')\n",
    "X_train=data['arr_0']\n",
    "y_train=data['arr_1']\n",
    "X_test=data['arr_2']\n",
    "y_test=data['arr_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, ..., 0.75, 0.5 , 1.  ],\n",
       "       [0.25, 0.5 , 0.5 , ..., 0.25, 0.75, 1.  ],\n",
       "       [0.25, 0.5 , 1.  , ..., 0.5 , 1.  , 0.5 ],\n",
       "       ...,\n",
       "       [0.5 , 0.25, 0.25, ..., 0.25, 1.  , 0.75],\n",
       "       [0.75, 1.  , 0.5 , ..., 0.25, 0.25, 0.75],\n",
       "       [1.  , 0.75, 1.  , ..., 1.  , 0.75, 1.  ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356863, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356863, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "y_train=y_train.reshape(y_train.shape[0])\n",
    "y_test=y_test.reshape(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356863,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 998, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 499, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 497, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 121, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               960500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               50200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,213,459\n",
      "Trainable params: 1,213,459\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# kernel size here might be changed to 2\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500,activation='relu'))\n",
    "model.add(Dense(250,activation='relu'))\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#opt=keras.optimizers.Adam(learning_rate=0.0001) # This learning rate is very low\n",
    "opt=keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "#model.compile(optimizer='adam',loss='mse')\n",
    "#model.compile(optimizer='adam',loss='BinaryCrossentropy')\n",
    "\n",
    "model.compile(optimizer=opt,loss='BinaryCrossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3569/3569 [==============================] - 867s 243ms/step - loss: 0.5709 - accuracy: 0.6873 - val_loss: 0.4913 - val_accuracy: 0.7681\n",
      "Epoch 2/300\n",
      "3569/3569 [==============================] - 927s 260ms/step - loss: 0.4458 - accuracy: 0.8004 - val_loss: 0.3882 - val_accuracy: 0.8332\n",
      "Epoch 3/300\n",
      "3569/3569 [==============================] - 934s 262ms/step - loss: 0.3669 - accuracy: 0.8461 - val_loss: 0.3147 - val_accuracy: 0.8701\n",
      "Epoch 4/300\n",
      "3569/3569 [==============================] - 953s 267ms/step - loss: 0.2764 - accuracy: 0.8879 - val_loss: 0.1908 - val_accuracy: 0.9269\n",
      "Epoch 5/300\n",
      "3569/3569 [==============================] - 912s 255ms/step - loss: 0.1936 - accuracy: 0.9247 - val_loss: 0.1475 - val_accuracy: 0.9459\n",
      "Epoch 6/300\n",
      "3569/3569 [==============================] - 886s 248ms/step - loss: 0.1418 - accuracy: 0.9475 - val_loss: 0.1030 - val_accuracy: 0.9653\n",
      "Epoch 7/300\n",
      "3569/3569 [==============================] - 892s 250ms/step - loss: 0.1102 - accuracy: 0.9607 - val_loss: 0.1320 - val_accuracy: 0.9491\n",
      "Epoch 8/300\n",
      "3569/3569 [==============================] - 901s 252ms/step - loss: 0.0914 - accuracy: 0.9680 - val_loss: 0.0726 - val_accuracy: 0.9762\n",
      "Epoch 9/300\n",
      "3569/3569 [==============================] - 31846s 9s/step - loss: 0.0728 - accuracy: 0.9755 - val_loss: 0.1034 - val_accuracy: 0.9638\n",
      "Epoch 10/300\n",
      "3569/3569 [==============================] - 833s 234ms/step - loss: 0.0589 - accuracy: 0.9803 - val_loss: 0.0729 - val_accuracy: 0.9757\n",
      "Epoch 11/300\n",
      "3569/3569 [==============================] - 869s 244ms/step - loss: 0.0467 - accuracy: 0.9846 - val_loss: 0.0533 - val_accuracy: 0.9825\n",
      "Epoch 12/300\n",
      "3569/3569 [==============================] - 878s 246ms/step - loss: 0.0363 - accuracy: 0.9882 - val_loss: 0.0501 - val_accuracy: 0.9839\n",
      "Epoch 13/300\n",
      "3569/3569 [==============================] - 876s 246ms/step - loss: 0.0307 - accuracy: 0.9900 - val_loss: 0.0386 - val_accuracy: 0.9876\n",
      "Epoch 14/300\n",
      "  84/3569 [..............................] - ETA: 10:13 - loss: 0.0164 - accuracy: 0.9953"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-182c352eec8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit(x=X_train,y=y_train,epochs=300,validation_data=(X_test,y_test),batch_size=100) \n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00373614, -0.07123931, -0.04291191, ...,  0.01957286,\n",
       "          0.09532103, -0.03212231],\n",
       "        [ 0.0277584 , -0.08137669,  0.07314269, ...,  0.05302334,\n",
       "         -0.00633785,  0.00817562],\n",
       "        [ 0.05358973, -0.10991233, -0.06330434, ..., -0.06649837,\n",
       "          0.10351663, -0.09555756],\n",
       "        ...,\n",
       "        [-0.08523045,  0.0473589 , -0.05680609, ...,  0.05038485,\n",
       "          0.06893742, -0.092932  ],\n",
       "        [-0.09635182, -0.0072091 ,  0.04101605, ...,  0.07698131,\n",
       "          0.04093489,  0.03832728],\n",
       "        [ 0.00310261, -0.01259931, -0.06683644, ..., -0.09701723,\n",
       "          0.16956215, -0.09296879]],\n",
       "\n",
       "       [[ 0.05205176,  0.0715901 , -0.082973  , ...,  0.07924911,\n",
       "          0.05529027,  0.09645613],\n",
       "        [ 0.05999482,  0.02433413, -0.00486802, ..., -0.01877996,\n",
       "          0.01912192, -0.06476969],\n",
       "        [ 0.05483685,  0.01884582,  0.02289651, ...,  0.04213466,\n",
       "         -0.07136921, -0.07843491],\n",
       "        ...,\n",
       "        [ 0.09658872, -0.05001483, -0.0318844 , ..., -0.07153062,\n",
       "          0.05825279, -0.00704403],\n",
       "        [ 0.04520487,  0.06377701,  0.08422805, ...,  0.02304448,\n",
       "          0.07561428, -0.07058069],\n",
       "        [ 0.03935722,  0.05132307,  0.09298541, ..., -0.16951449,\n",
       "          0.08504364, -0.04782554]],\n",
       "\n",
       "       [[-0.05057992, -0.0574871 , -0.00055592, ...,  0.06624529,\n",
       "          0.03776521,  0.06746821],\n",
       "        [-0.02478649,  0.08355421,  0.06871179, ...,  0.01461647,\n",
       "          0.07926257, -0.0302128 ],\n",
       "        [-0.05251868,  0.00513634,  0.0269939 , ..., -0.11192942,\n",
       "          0.09868197, -0.02382536],\n",
       "        ...,\n",
       "        [-0.09226557,  0.07056183,  0.07633366, ..., -0.09931305,\n",
       "          0.02252094, -0.03654494],\n",
       "        [ 0.00678859,  0.09432916, -0.1046863 , ...,  0.08949436,\n",
       "         -0.01139162, -0.09263526],\n",
       "        [ 0.06713904, -0.07358874, -0.02479492, ..., -0.07514193,\n",
       "          0.10024594,  0.06441599]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=weights[0] #this should give the weights of the first layer in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.03271734e-02, -1.42122403e-01,  1.64065093e-01,\n",
       "          1.01419061e-01, -8.70682821e-02, -2.45525800e-02,\n",
       "          5.28803049e-03,  7.85778537e-02,  1.31431520e-01,\n",
       "         -6.31649271e-02,  2.87806969e-02, -6.69077784e-02,\n",
       "         -1.68675676e-01,  5.25860395e-03, -1.79913174e-02,\n",
       "          1.08323004e-02, -8.75546113e-02, -1.00276932e-01,\n",
       "          2.50561861e-03,  1.64780952e-02, -1.68976456e-01,\n",
       "          1.71001971e-01,  1.41400665e-01, -1.35098502e-01,\n",
       "          1.63141966e-01,  6.60270602e-02, -6.52495995e-02,\n",
       "          5.78200892e-02, -1.06512301e-01,  1.11497782e-01,\n",
       "          1.20252166e-02,  1.20029869e-02, -2.21912023e-02,\n",
       "         -1.20340571e-01, -3.38289924e-02, -4.13323753e-02,\n",
       "         -8.93164799e-02,  1.92399845e-01, -1.25866935e-01,\n",
       "         -1.58961296e-01,  1.54569829e-02,  3.76193933e-02,\n",
       "         -5.61960116e-02, -4.83778864e-02,  1.62057072e-01,\n",
       "         -1.90857306e-01, -1.46048427e-01, -1.02465063e-01,\n",
       "          1.85697600e-01,  1.16884701e-01, -1.39794260e-01,\n",
       "         -3.41604389e-02,  9.93944332e-02,  5.74022606e-02,\n",
       "          1.25784650e-01,  4.57796492e-02, -1.46773726e-01,\n",
       "          1.14726901e-01, -2.90926136e-02, -6.75545260e-02,\n",
       "          9.56175476e-02, -6.07435545e-03, -4.67778333e-02,\n",
       "         -1.41924247e-01]],\n",
       "\n",
       "       [[-1.13496393e-01, -1.55593276e-01, -1.48717955e-01,\n",
       "         -9.10867304e-02,  8.89023542e-02, -1.06300265e-01,\n",
       "          1.87531319e-02,  1.29638791e-01,  8.11186898e-03,\n",
       "         -7.71652237e-02, -4.23873179e-02, -1.32298157e-01,\n",
       "         -8.96516219e-02,  1.33351043e-01, -1.62258342e-01,\n",
       "          5.24431989e-02,  1.04082525e-02, -4.56772745e-03,\n",
       "          1.93186067e-02,  1.47221342e-01,  1.05555438e-01,\n",
       "         -1.61862329e-01, -1.35988086e-01, -8.84664059e-02,\n",
       "         -3.96517403e-02,  3.53493169e-02, -4.86836433e-02,\n",
       "         -3.15463729e-02,  1.58225209e-01,  7.21950009e-02,\n",
       "         -1.14128239e-01,  1.14383228e-01,  1.03620209e-01,\n",
       "         -2.01840624e-02, -2.04426095e-01, -1.61297619e-01,\n",
       "          1.44101202e-01, -1.22737102e-01,  1.59477904e-01,\n",
       "         -1.71999395e-01,  1.41454697e-01, -1.17175885e-01,\n",
       "          1.23646662e-01, -4.47544903e-02, -8.65921900e-02,\n",
       "         -1.20569289e-01,  4.68562841e-02, -1.34037331e-01,\n",
       "         -1.47391826e-01, -1.33132309e-01,  1.28239721e-01,\n",
       "         -1.40906453e-01, -4.71871532e-02, -1.21396899e-01,\n",
       "          1.44366029e-04,  7.04243407e-02, -1.04652168e-02,\n",
       "          4.62930314e-02,  5.50052933e-02,  1.56623706e-01,\n",
       "         -1.35241738e-02, -6.04158617e-04,  2.30038408e-02,\n",
       "         -1.41061638e-02]],\n",
       "\n",
       "       [[ 1.37004077e-01,  1.25138462e-03,  5.66673130e-02,\n",
       "          3.06219590e-05, -1.33735731e-01,  1.46359995e-01,\n",
       "          4.19711461e-03,  2.01987550e-02, -1.15534328e-01,\n",
       "          1.73693448e-02,  6.26672432e-02,  6.58512935e-02,\n",
       "         -6.75296187e-02, -1.93718038e-02,  1.56756192e-01,\n",
       "          6.62690848e-02, -1.80429220e-02, -5.54526448e-02,\n",
       "          1.52284488e-01, -1.34793326e-01,  3.04241404e-02,\n",
       "         -6.12087213e-02, -4.88331541e-02, -6.23112693e-02,\n",
       "         -2.14480355e-01, -8.23532790e-03,  1.20597035e-02,\n",
       "          1.18821293e-01, -2.79551353e-02,  1.07017592e-01,\n",
       "          5.45856357e-02, -8.91348813e-03,  1.19134292e-01,\n",
       "          1.41446412e-01,  1.00391425e-01,  1.48963034e-01,\n",
       "         -4.19385619e-02, -1.10571317e-01, -1.59678102e-01,\n",
       "          1.62266448e-01, -4.70891595e-02,  1.00826569e-01,\n",
       "          8.79886448e-02, -1.73730835e-01,  7.93008581e-02,\n",
       "          1.90216228e-01, -2.70252358e-02, -1.05662197e-02,\n",
       "         -1.07581019e-01,  2.89094802e-02, -1.35056734e-01,\n",
       "          1.19659275e-01, -6.09339960e-02,  6.75225556e-02,\n",
       "         -1.45446017e-01,  4.62302528e-02,  1.19520403e-01,\n",
       "         -1.15793720e-01,  1.54092416e-01,  8.45776349e-02,\n",
       "          3.29489410e-02,  1.55198038e-01,  1.45549178e-01,\n",
       "          1.09755546e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1w = model.get_weights()[0][:,:] #not sure if this [0] is needed here\n",
    "for i in range(1,26):\n",
    "    plt.subplot(5,5,i)\n",
    "    plt.imshow(x1w[:,:,i],interpolation=\"nearest\",cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df=pd.DataFrame(model.history.history)\n",
    "loss_df.to_csv('partial_trained_thousandBPmodel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.743577</td>\n",
       "      <td>0.491264</td>\n",
       "      <td>0.768085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.427422</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.388238</td>\n",
       "      <td>0.833163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.346895</td>\n",
       "      <td>0.855639</td>\n",
       "      <td>0.314708</td>\n",
       "      <td>0.870085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.255485</td>\n",
       "      <td>0.897241</td>\n",
       "      <td>0.190771</td>\n",
       "      <td>0.926880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180726</td>\n",
       "      <td>0.930223</td>\n",
       "      <td>0.147490</td>\n",
       "      <td>0.945870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.136399</td>\n",
       "      <td>0.949950</td>\n",
       "      <td>0.103042</td>\n",
       "      <td>0.965298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.106394</td>\n",
       "      <td>0.962173</td>\n",
       "      <td>0.132042</td>\n",
       "      <td>0.949132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.087772</td>\n",
       "      <td>0.969554</td>\n",
       "      <td>0.072628</td>\n",
       "      <td>0.976201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.070785</td>\n",
       "      <td>0.976061</td>\n",
       "      <td>0.103415</td>\n",
       "      <td>0.963818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058958</td>\n",
       "      <td>0.980320</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.975741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.984151</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>0.982548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.987312</td>\n",
       "      <td>0.050081</td>\n",
       "      <td>0.983929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.989032</td>\n",
       "      <td>0.038556</td>\n",
       "      <td>0.987623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.516835  0.743577  0.491264      0.768085\n",
       "1   0.427422  0.812062  0.388238      0.833163\n",
       "2   0.346895  0.855639  0.314708      0.870085\n",
       "3   0.255485  0.897241  0.190771      0.926880\n",
       "4   0.180726  0.930223  0.147490      0.945870\n",
       "5   0.136399  0.949950  0.103042      0.965298\n",
       "6   0.106394  0.962173  0.132042      0.949132\n",
       "7   0.087772  0.969554  0.072628      0.976201\n",
       "8   0.070785  0.976061  0.103415      0.963818\n",
       "9   0.058958  0.980320  0.072885      0.975741\n",
       "10  0.047781  0.984151  0.053339      0.982548\n",
       "11  0.038895  0.987312  0.050081      0.983929\n",
       "12  0.033118  0.989032  0.038556      0.987623"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model=load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3569/3569 [==============================] - 878s 246ms/step - loss: 0.0289 - accuracy: 0.9906 - val_loss: 0.0425 - val_accuracy: 0.9852\n",
      "Epoch 2/300\n",
      "3569/3569 [==============================] - 971s 272ms/step - loss: 0.0232 - accuracy: 0.9924 - val_loss: 0.0464 - val_accuracy: 0.9835\n",
      "Epoch 3/300\n",
      "3569/3569 [==============================] - 974s 273ms/step - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.0704 - val_accuracy: 0.9796\n",
      "Epoch 4/300\n",
      "3569/3569 [==============================] - 1028s 288ms/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 0.0456 - val_accuracy: 0.9863\n",
      "Epoch 5/300\n",
      "3569/3569 [==============================] - 944s 265ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.0945 - val_accuracy: 0.9741\n",
      "Epoch 6/300\n",
      "3569/3569 [==============================] - 947s 265ms/step - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.0512 - val_accuracy: 0.9862\n",
      "Epoch 7/300\n",
      "3569/3569 [==============================] - 980s 275ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0309 - val_accuracy: 0.9908\n",
      "Epoch 8/300\n",
      "3569/3569 [==============================] - 973s 273ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.0516 - val_accuracy: 0.9868\n",
      "Epoch 9/300\n",
      "3569/3569 [==============================] - 960s 269ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0315 - val_accuracy: 0.9907\n",
      "Epoch 10/300\n",
      "3569/3569 [==============================] - 1000s 280ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0410 - val_accuracy: 0.9894\n",
      "Epoch 11/300\n",
      "3569/3569 [==============================] - 999s 280ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 0.0491 - val_accuracy: 0.9870\n",
      "Epoch 12/300\n",
      "3569/3569 [==============================] - 969s 271ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0284 - val_accuracy: 0.9917\n",
      "Epoch 13/300\n",
      "3569/3569 [==============================] - 975s 273ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0437 - val_accuracy: 0.9892\n",
      "Epoch 14/300\n",
      "3569/3569 [==============================] - 965s 270ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.0375 - val_accuracy: 0.9894\n",
      "Epoch 15/300\n",
      "3569/3569 [==============================] - 940s 263ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0604 - val_accuracy: 0.9826\n",
      "Epoch 16/300\n",
      "3569/3569 [==============================] - 943s 264ms/step - loss: 0.0074 - accuracy: 0.9975 - val_loss: 0.0443 - val_accuracy: 0.9877\n",
      "Epoch 17/300\n",
      "3310/3569 [==========================>...] - ETA: 52s - loss: 0.0079 - accuracy: 0.9973 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d89d91d70032>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=new_model.fit(x=X_train,y=y_train,epochs=300,validation_data=(X_test,y_test),batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save('my_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loss_df=pd.DataFrame(new_model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loss_df.to_csv('new_partial_trained_thousandBPmodel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights=new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028911</td>\n",
       "      <td>0.990579</td>\n",
       "      <td>0.042532</td>\n",
       "      <td>0.985154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023191</td>\n",
       "      <td>0.992426</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.983531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019795</td>\n",
       "      <td>0.993454</td>\n",
       "      <td>0.070365</td>\n",
       "      <td>0.979606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018906</td>\n",
       "      <td>0.993541</td>\n",
       "      <td>0.045566</td>\n",
       "      <td>0.986328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.994852</td>\n",
       "      <td>0.094505</td>\n",
       "      <td>0.974080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.994995</td>\n",
       "      <td>0.051180</td>\n",
       "      <td>0.986188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.996004</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>0.990789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.051597</td>\n",
       "      <td>0.986847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.996099</td>\n",
       "      <td>0.031511</td>\n",
       "      <td>0.990680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.996542</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>0.989436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>0.049138</td>\n",
       "      <td>0.986964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.996954</td>\n",
       "      <td>0.028361</td>\n",
       "      <td>0.991722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>0.989231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.009047</td>\n",
       "      <td>0.996870</td>\n",
       "      <td>0.037512</td>\n",
       "      <td>0.989363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.996850</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>0.982593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.997520</td>\n",
       "      <td>0.044324</td>\n",
       "      <td>0.987690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.028911  0.990579  0.042532      0.985154\n",
       "1   0.023191  0.992426  0.046401      0.983531\n",
       "2   0.019795  0.993454  0.070365      0.979606\n",
       "3   0.018906  0.993541  0.045566      0.986328\n",
       "4   0.015225  0.994852  0.094505      0.974080\n",
       "5   0.014290  0.994995  0.051180      0.986188\n",
       "6   0.012047  0.996004  0.030915      0.990789\n",
       "7   0.011652  0.996094  0.051597      0.986847\n",
       "8   0.011799  0.996099  0.031511      0.990680\n",
       "9   0.010440  0.996542  0.040986      0.989436\n",
       "10  0.010490  0.996354  0.049138      0.986964\n",
       "11  0.009479  0.996954  0.028361      0.991722\n",
       "12  0.009459  0.996895  0.043721      0.989231\n",
       "13  0.009047  0.996870  0.037512      0.989363\n",
       "14  0.008934  0.996850  0.060384      0.982593\n",
       "15  0.007439  0.997520  0.044324      0.987690"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_2=load_model('my_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3569/3569 [==============================] - 1441s 404ms/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 0.0576 - val_accuracy: 0.9789\n",
      "Epoch 2/300\n",
      "3569/3569 [==============================] - 951s 266ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 0.0297 - val_accuracy: 0.9918\n",
      "Epoch 3/300\n",
      "  60/3569 [..............................] - ETA: 15:18 - loss: 0.0028 - accuracy: 0.9992"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-b24aceb78a75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_model_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \"\"\"\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1020\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1083\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1085\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m       \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m       \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimport_lock_held\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mevt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    414\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    415\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=new_model_2.fit(x=X_train,y=y_train,epochs=300,validation_data=(X_test,y_test),batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_2.save('my_new_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loss_df_2=pd.DataFrame(new_model_2.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loss_df_2.to_csv('new_partial_trained_thousandBPmodel_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights=new_model_2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.057569</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.997285</td>\n",
       "      <td>0.029673</td>\n",
       "      <td>0.991792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.007758  0.997307  0.057569      0.978908\n",
       "1  0.007457  0.997285  0.029673      0.991792"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_3=load_model('my_new_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3569/3569 [==============================] - 842s 236ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.0557 - val_accuracy: 0.9811\n",
      "Epoch 2/300\n",
      "3569/3569 [==============================] - 900s 252ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.0317 - val_accuracy: 0.9915\n",
      "Epoch 3/300\n",
      "3569/3569 [==============================] - 910s 255ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0349 - val_accuracy: 0.9894\n",
      "Epoch 4/300\n",
      "3569/3569 [==============================] - 902s 253ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0372 - val_accuracy: 0.9913\n",
      "Epoch 5/300\n",
      "3569/3569 [==============================] - 907s 254ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0386 - val_accuracy: 0.9909\n",
      "Epoch 6/300\n",
      "3569/3569 [==============================] - 898s 252ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0246 - val_accuracy: 0.9937\n",
      "Epoch 7/300\n",
      "3569/3569 [==============================] - 895s 251ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.0381 - val_accuracy: 0.9904\n",
      "Epoch 8/300\n",
      "3569/3569 [==============================] - 904s 253ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0556 - val_accuracy: 0.9814\n",
      "Epoch 9/300\n",
      "3569/3569 [==============================] - 896s 251ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0341 - val_accuracy: 0.9919\n",
      "Epoch 10/300\n",
      "3569/3569 [==============================] - 893s 250ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0240 - val_accuracy: 0.9932\n",
      "Epoch 11/300\n",
      "3569/3569 [==============================] - 898s 251ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0442 - val_accuracy: 0.9896\n",
      "Epoch 12/300\n",
      "3569/3569 [==============================] - 896s 251ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0336 - val_accuracy: 0.9931\n",
      "Epoch 13/300\n",
      "3569/3569 [==============================] - 898s 252ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0558 - val_accuracy: 0.9868\n",
      "Epoch 14/300\n",
      "3569/3569 [==============================] - 906s 254ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0235 - val_accuracy: 0.9937\n",
      "Epoch 15/300\n",
      "3569/3569 [==============================] - 899s 252ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0276 - val_accuracy: 0.9936\n",
      "Epoch 16/300\n",
      "3569/3569 [==============================] - 892s 250ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0254 - val_accuracy: 0.9931\n",
      "Epoch 17/300\n",
      "3569/3569 [==============================] - 904s 253ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0499 - val_accuracy: 0.9851\n",
      "Epoch 18/300\n",
      "3569/3569 [==============================] - 932s 261ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0356 - val_accuracy: 0.9875\n",
      "Epoch 19/300\n",
      "3569/3569 [==============================] - 898s 252ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0468 - val_accuracy: 0.9905\n",
      "Epoch 20/300\n",
      "3569/3569 [==============================] - 896s 251ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0480 - val_accuracy: 0.9908\n",
      "Epoch 21/300\n",
      "3569/3569 [==============================] - 905s 254ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0304 - val_accuracy: 0.9927\n",
      "Epoch 22/300\n",
      "3569/3569 [==============================] - 900s 252ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0223 - val_accuracy: 0.9947\n",
      "Epoch 23/300\n",
      "3569/3569 [==============================] - 898s 252ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0319 - val_accuracy: 0.9919\n",
      "Epoch 24/300\n",
      "3569/3569 [==============================] - 896s 251ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0273 - val_accuracy: 0.9923\n",
      "Epoch 25/300\n",
      "3569/3569 [==============================] - 900s 252ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0350 - val_accuracy: 0.9926\n",
      "Epoch 26/300\n",
      "3569/3569 [==============================] - 893s 250ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0228 - val_accuracy: 0.9932\n",
      "Epoch 27/300\n",
      "3569/3569 [==============================] - 911s 255ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0285 - val_accuracy: 0.9933\n",
      "Epoch 28/300\n",
      "1331/3569 [==========>...................] - ETA: 7:32 - loss: 0.0026 - accuracy: 0.9992"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-a07539403fa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_model_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=new_model_3.fit(x=X_train,y=y_train,epochs=300,validation_data=(X_test,y_test),batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.055699</td>\n",
       "      <td>0.981127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.997571</td>\n",
       "      <td>0.031687</td>\n",
       "      <td>0.991526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.989385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005651</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.991254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>0.990851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.998086</td>\n",
       "      <td>0.024568</td>\n",
       "      <td>0.993664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>0.990355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.998092</td>\n",
       "      <td>0.055615</td>\n",
       "      <td>0.981441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.998307</td>\n",
       "      <td>0.034108</td>\n",
       "      <td>0.991862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.993227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.998490</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>0.989635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.998442</td>\n",
       "      <td>0.033585</td>\n",
       "      <td>0.993107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.997985</td>\n",
       "      <td>0.055798</td>\n",
       "      <td>0.986847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>0.993703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.993603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.998621</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.993121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.998537</td>\n",
       "      <td>0.049858</td>\n",
       "      <td>0.985098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.035581</td>\n",
       "      <td>0.987541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.998644</td>\n",
       "      <td>0.046769</td>\n",
       "      <td>0.990495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.998753</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.990756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.030351</td>\n",
       "      <td>0.992742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.998574</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.994718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.998848</td>\n",
       "      <td>0.031905</td>\n",
       "      <td>0.991930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.998860</td>\n",
       "      <td>0.027323</td>\n",
       "      <td>0.992302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.992616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.998778</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.993193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.993286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.005988  0.997938  0.055699      0.981127\n",
       "1   0.007042  0.997571  0.031687      0.991526\n",
       "2   0.006263  0.997924  0.034874      0.989385\n",
       "3   0.005651  0.998125  0.037228      0.991254\n",
       "4   0.006479  0.997848  0.038642      0.990851\n",
       "5   0.005737  0.998086  0.024568      0.993664\n",
       "6   0.005101  0.998265  0.038149      0.990355\n",
       "7   0.005682  0.998092  0.055615      0.981441\n",
       "8   0.005025  0.998307  0.034108      0.991862\n",
       "9   0.005829  0.998221  0.023984      0.993227\n",
       "10  0.004662  0.998490  0.044173      0.989635\n",
       "11  0.004576  0.998442  0.033585      0.993107\n",
       "12  0.006720  0.997985  0.055798      0.986847\n",
       "13  0.003737  0.998750  0.023518      0.993703\n",
       "14  0.005260  0.998221  0.027622      0.993603\n",
       "15  0.004205  0.998621  0.025375      0.993121\n",
       "16  0.004536  0.998537  0.049858      0.985098\n",
       "17  0.004182  0.998560  0.035581      0.987541\n",
       "18  0.003930  0.998644  0.046769      0.990495\n",
       "19  0.003843  0.998753  0.048027      0.990756\n",
       "20  0.004112  0.998633  0.030351      0.992742\n",
       "21  0.004188  0.998574  0.022256      0.994718\n",
       "22  0.003368  0.998848  0.031905      0.991930\n",
       "23  0.003585  0.998860  0.027323      0.992302\n",
       "24  0.003252  0.999000  0.035009      0.992616\n",
       "25  0.003624  0.998778  0.022789      0.993193\n",
       "26  0.003400  0.998904  0.028461      0.993286"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_3.save('my_new_model_3.h5')\n",
    "new_loss_df_3=pd.DataFrame(new_model_3.history.history)\n",
    "new_loss_df_3.to_csv('new_partial_trained_thousandBPmodel_3.csv')\n",
    "new_weights_3=new_model_3.get_weights()\n",
    "new_loss_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.743577</td>\n",
       "      <td>0.491264</td>\n",
       "      <td>0.768085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.427422</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.388238</td>\n",
       "      <td>0.833163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.346895</td>\n",
       "      <td>0.855639</td>\n",
       "      <td>0.314708</td>\n",
       "      <td>0.870085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.255485</td>\n",
       "      <td>0.897241</td>\n",
       "      <td>0.190771</td>\n",
       "      <td>0.926880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180726</td>\n",
       "      <td>0.930223</td>\n",
       "      <td>0.147490</td>\n",
       "      <td>0.945870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.136399</td>\n",
       "      <td>0.949950</td>\n",
       "      <td>0.103042</td>\n",
       "      <td>0.965298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.106394</td>\n",
       "      <td>0.962173</td>\n",
       "      <td>0.132042</td>\n",
       "      <td>0.949132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.087772</td>\n",
       "      <td>0.969554</td>\n",
       "      <td>0.072628</td>\n",
       "      <td>0.976201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.070785</td>\n",
       "      <td>0.976061</td>\n",
       "      <td>0.103415</td>\n",
       "      <td>0.963818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058958</td>\n",
       "      <td>0.980320</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.975741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.984151</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>0.982548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.987312</td>\n",
       "      <td>0.050081</td>\n",
       "      <td>0.983929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.989032</td>\n",
       "      <td>0.038556</td>\n",
       "      <td>0.987623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.516835  0.743577  0.491264      0.768085\n",
       "1   0.427422  0.812062  0.388238      0.833163\n",
       "2   0.346895  0.855639  0.314708      0.870085\n",
       "3   0.255485  0.897241  0.190771      0.926880\n",
       "4   0.180726  0.930223  0.147490      0.945870\n",
       "5   0.136399  0.949950  0.103042      0.965298\n",
       "6   0.106394  0.962173  0.132042      0.949132\n",
       "7   0.087772  0.969554  0.072628      0.976201\n",
       "8   0.070785  0.976061  0.103415      0.963818\n",
       "9   0.058958  0.980320  0.072885      0.975741\n",
       "10  0.047781  0.984151  0.053339      0.982548\n",
       "11  0.038895  0.987312  0.050081      0.983929\n",
       "12  0.033118  0.989032  0.038556      0.987623"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028911</td>\n",
       "      <td>0.990579</td>\n",
       "      <td>0.042532</td>\n",
       "      <td>0.985154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023191</td>\n",
       "      <td>0.992426</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.983531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019795</td>\n",
       "      <td>0.993454</td>\n",
       "      <td>0.070365</td>\n",
       "      <td>0.979606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018906</td>\n",
       "      <td>0.993541</td>\n",
       "      <td>0.045566</td>\n",
       "      <td>0.986328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.994852</td>\n",
       "      <td>0.094505</td>\n",
       "      <td>0.974080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.994995</td>\n",
       "      <td>0.051180</td>\n",
       "      <td>0.986188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.996004</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>0.990789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.051597</td>\n",
       "      <td>0.986847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.996099</td>\n",
       "      <td>0.031511</td>\n",
       "      <td>0.990680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.996542</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>0.989436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>0.049138</td>\n",
       "      <td>0.986964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.996954</td>\n",
       "      <td>0.028361</td>\n",
       "      <td>0.991722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>0.989231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.009047</td>\n",
       "      <td>0.996870</td>\n",
       "      <td>0.037512</td>\n",
       "      <td>0.989363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.996850</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>0.982593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.997520</td>\n",
       "      <td>0.044324</td>\n",
       "      <td>0.987690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.028911  0.990579  0.042532      0.985154\n",
       "1   0.023191  0.992426  0.046401      0.983531\n",
       "2   0.019795  0.993454  0.070365      0.979606\n",
       "3   0.018906  0.993541  0.045566      0.986328\n",
       "4   0.015225  0.994852  0.094505      0.974080\n",
       "5   0.014290  0.994995  0.051180      0.986188\n",
       "6   0.012047  0.996004  0.030915      0.990789\n",
       "7   0.011652  0.996094  0.051597      0.986847\n",
       "8   0.011799  0.996099  0.031511      0.990680\n",
       "9   0.010440  0.996542  0.040986      0.989436\n",
       "10  0.010490  0.996354  0.049138      0.986964\n",
       "11  0.009479  0.996954  0.028361      0.991722\n",
       "12  0.009459  0.996895  0.043721      0.989231\n",
       "13  0.009047  0.996870  0.037512      0.989363\n",
       "14  0.008934  0.996850  0.060384      0.982593\n",
       "15  0.007439  0.997520  0.044324      0.987690"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.057569</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.997285</td>\n",
       "      <td>0.029673</td>\n",
       "      <td>0.991792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.007758  0.997307  0.057569      0.978908\n",
       "1  0.007457  0.997285  0.029673      0.991792"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.055699</td>\n",
       "      <td>0.981127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.997571</td>\n",
       "      <td>0.031687</td>\n",
       "      <td>0.991526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.989385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005651</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.991254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>0.990851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.998086</td>\n",
       "      <td>0.024568</td>\n",
       "      <td>0.993664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>0.990355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.998092</td>\n",
       "      <td>0.055615</td>\n",
       "      <td>0.981441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.998307</td>\n",
       "      <td>0.034108</td>\n",
       "      <td>0.991862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.993227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.998490</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>0.989635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.998442</td>\n",
       "      <td>0.033585</td>\n",
       "      <td>0.993107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.997985</td>\n",
       "      <td>0.055798</td>\n",
       "      <td>0.986847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>0.993703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.993603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.998621</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.993121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.998537</td>\n",
       "      <td>0.049858</td>\n",
       "      <td>0.985098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.035581</td>\n",
       "      <td>0.987541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.998644</td>\n",
       "      <td>0.046769</td>\n",
       "      <td>0.990495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.998753</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.990756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.030351</td>\n",
       "      <td>0.992742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.998574</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.994718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.998848</td>\n",
       "      <td>0.031905</td>\n",
       "      <td>0.991930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.998860</td>\n",
       "      <td>0.027323</td>\n",
       "      <td>0.992302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.992616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.998778</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.993193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.993286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.005988  0.997938  0.055699      0.981127\n",
       "1   0.007042  0.997571  0.031687      0.991526\n",
       "2   0.006263  0.997924  0.034874      0.989385\n",
       "3   0.005651  0.998125  0.037228      0.991254\n",
       "4   0.006479  0.997848  0.038642      0.990851\n",
       "5   0.005737  0.998086  0.024568      0.993664\n",
       "6   0.005101  0.998265  0.038149      0.990355\n",
       "7   0.005682  0.998092  0.055615      0.981441\n",
       "8   0.005025  0.998307  0.034108      0.991862\n",
       "9   0.005829  0.998221  0.023984      0.993227\n",
       "10  0.004662  0.998490  0.044173      0.989635\n",
       "11  0.004576  0.998442  0.033585      0.993107\n",
       "12  0.006720  0.997985  0.055798      0.986847\n",
       "13  0.003737  0.998750  0.023518      0.993703\n",
       "14  0.005260  0.998221  0.027622      0.993603\n",
       "15  0.004205  0.998621  0.025375      0.993121\n",
       "16  0.004536  0.998537  0.049858      0.985098\n",
       "17  0.004182  0.998560  0.035581      0.987541\n",
       "18  0.003930  0.998644  0.046769      0.990495\n",
       "19  0.003843  0.998753  0.048027      0.990756\n",
       "20  0.004112  0.998633  0.030351      0.992742\n",
       "21  0.004188  0.998574  0.022256      0.994718\n",
       "22  0.003368  0.998848  0.031905      0.991930\n",
       "23  0.003585  0.998860  0.027323      0.992302\n",
       "24  0.003252  0.999000  0.035009      0.992616\n",
       "25  0.003624  0.998778  0.022789      0.993193\n",
       "26  0.003400  0.998904  0.028461      0.993286"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df=pd.concat([loss_df,new_loss_df,new_loss_df_2,new_loss_df_3],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.743577</td>\n",
       "      <td>0.491264</td>\n",
       "      <td>0.768085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.427422</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.388238</td>\n",
       "      <td>0.833163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.346895</td>\n",
       "      <td>0.855639</td>\n",
       "      <td>0.314708</td>\n",
       "      <td>0.870085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.255485</td>\n",
       "      <td>0.897241</td>\n",
       "      <td>0.190771</td>\n",
       "      <td>0.926880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180726</td>\n",
       "      <td>0.930223</td>\n",
       "      <td>0.147490</td>\n",
       "      <td>0.945870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.136399</td>\n",
       "      <td>0.949950</td>\n",
       "      <td>0.103042</td>\n",
       "      <td>0.965298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.106394</td>\n",
       "      <td>0.962173</td>\n",
       "      <td>0.132042</td>\n",
       "      <td>0.949132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.087772</td>\n",
       "      <td>0.969554</td>\n",
       "      <td>0.072628</td>\n",
       "      <td>0.976201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.070785</td>\n",
       "      <td>0.976061</td>\n",
       "      <td>0.103415</td>\n",
       "      <td>0.963818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058958</td>\n",
       "      <td>0.980320</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.975741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.984151</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>0.982548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.987312</td>\n",
       "      <td>0.050081</td>\n",
       "      <td>0.983929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.989032</td>\n",
       "      <td>0.038556</td>\n",
       "      <td>0.987623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028911</td>\n",
       "      <td>0.990579</td>\n",
       "      <td>0.042532</td>\n",
       "      <td>0.985154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023191</td>\n",
       "      <td>0.992426</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.983531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019795</td>\n",
       "      <td>0.993454</td>\n",
       "      <td>0.070365</td>\n",
       "      <td>0.979606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018906</td>\n",
       "      <td>0.993541</td>\n",
       "      <td>0.045566</td>\n",
       "      <td>0.986328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.994852</td>\n",
       "      <td>0.094505</td>\n",
       "      <td>0.974080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.994995</td>\n",
       "      <td>0.051180</td>\n",
       "      <td>0.986188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.996004</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>0.990789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.051597</td>\n",
       "      <td>0.986847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.996099</td>\n",
       "      <td>0.031511</td>\n",
       "      <td>0.990680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.996542</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>0.989436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>0.049138</td>\n",
       "      <td>0.986964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.996954</td>\n",
       "      <td>0.028361</td>\n",
       "      <td>0.991722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>0.989231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.009047</td>\n",
       "      <td>0.996870</td>\n",
       "      <td>0.037512</td>\n",
       "      <td>0.989363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.996850</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>0.982593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.997520</td>\n",
       "      <td>0.044324</td>\n",
       "      <td>0.987690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.057569</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.997285</td>\n",
       "      <td>0.029673</td>\n",
       "      <td>0.991792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.055699</td>\n",
       "      <td>0.981127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.997571</td>\n",
       "      <td>0.031687</td>\n",
       "      <td>0.991526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.989385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005651</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.991254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>0.990851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.998086</td>\n",
       "      <td>0.024568</td>\n",
       "      <td>0.993664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>0.990355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.998092</td>\n",
       "      <td>0.055615</td>\n",
       "      <td>0.981441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.998307</td>\n",
       "      <td>0.034108</td>\n",
       "      <td>0.991862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.993227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.998490</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>0.989635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.998442</td>\n",
       "      <td>0.033585</td>\n",
       "      <td>0.993107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.997985</td>\n",
       "      <td>0.055798</td>\n",
       "      <td>0.986847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>0.993703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.993603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.998621</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.993121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.998537</td>\n",
       "      <td>0.049858</td>\n",
       "      <td>0.985098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.035581</td>\n",
       "      <td>0.987541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.998644</td>\n",
       "      <td>0.046769</td>\n",
       "      <td>0.990495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.998753</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.990756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.030351</td>\n",
       "      <td>0.992742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.998574</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.994718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.998848</td>\n",
       "      <td>0.031905</td>\n",
       "      <td>0.991930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.998860</td>\n",
       "      <td>0.027323</td>\n",
       "      <td>0.992302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.992616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.998778</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.993193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.993286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.516835  0.743577  0.491264      0.768085\n",
       "1   0.427422  0.812062  0.388238      0.833163\n",
       "2   0.346895  0.855639  0.314708      0.870085\n",
       "3   0.255485  0.897241  0.190771      0.926880\n",
       "4   0.180726  0.930223  0.147490      0.945870\n",
       "5   0.136399  0.949950  0.103042      0.965298\n",
       "6   0.106394  0.962173  0.132042      0.949132\n",
       "7   0.087772  0.969554  0.072628      0.976201\n",
       "8   0.070785  0.976061  0.103415      0.963818\n",
       "9   0.058958  0.980320  0.072885      0.975741\n",
       "10  0.047781  0.984151  0.053339      0.982548\n",
       "11  0.038895  0.987312  0.050081      0.983929\n",
       "12  0.033118  0.989032  0.038556      0.987623\n",
       "0   0.028911  0.990579  0.042532      0.985154\n",
       "1   0.023191  0.992426  0.046401      0.983531\n",
       "2   0.019795  0.993454  0.070365      0.979606\n",
       "3   0.018906  0.993541  0.045566      0.986328\n",
       "4   0.015225  0.994852  0.094505      0.974080\n",
       "5   0.014290  0.994995  0.051180      0.986188\n",
       "6   0.012047  0.996004  0.030915      0.990789\n",
       "7   0.011652  0.996094  0.051597      0.986847\n",
       "8   0.011799  0.996099  0.031511      0.990680\n",
       "9   0.010440  0.996542  0.040986      0.989436\n",
       "10  0.010490  0.996354  0.049138      0.986964\n",
       "11  0.009479  0.996954  0.028361      0.991722\n",
       "12  0.009459  0.996895  0.043721      0.989231\n",
       "13  0.009047  0.996870  0.037512      0.989363\n",
       "14  0.008934  0.996850  0.060384      0.982593\n",
       "15  0.007439  0.997520  0.044324      0.987690\n",
       "0   0.007758  0.997307  0.057569      0.978908\n",
       "1   0.007457  0.997285  0.029673      0.991792\n",
       "0   0.005988  0.997938  0.055699      0.981127\n",
       "1   0.007042  0.997571  0.031687      0.991526\n",
       "2   0.006263  0.997924  0.034874      0.989385\n",
       "3   0.005651  0.998125  0.037228      0.991254\n",
       "4   0.006479  0.997848  0.038642      0.990851\n",
       "5   0.005737  0.998086  0.024568      0.993664\n",
       "6   0.005101  0.998265  0.038149      0.990355\n",
       "7   0.005682  0.998092  0.055615      0.981441\n",
       "8   0.005025  0.998307  0.034108      0.991862\n",
       "9   0.005829  0.998221  0.023984      0.993227\n",
       "10  0.004662  0.998490  0.044173      0.989635\n",
       "11  0.004576  0.998442  0.033585      0.993107\n",
       "12  0.006720  0.997985  0.055798      0.986847\n",
       "13  0.003737  0.998750  0.023518      0.993703\n",
       "14  0.005260  0.998221  0.027622      0.993603\n",
       "15  0.004205  0.998621  0.025375      0.993121\n",
       "16  0.004536  0.998537  0.049858      0.985098\n",
       "17  0.004182  0.998560  0.035581      0.987541\n",
       "18  0.003930  0.998644  0.046769      0.990495\n",
       "19  0.003843  0.998753  0.048027      0.990756\n",
       "20  0.004112  0.998633  0.030351      0.992742\n",
       "21  0.004188  0.998574  0.022256      0.994718\n",
       "22  0.003368  0.998848  0.031905      0.991930\n",
       "23  0.003585  0.998860  0.027323      0.992302\n",
       "24  0.003252  0.999000  0.035009      0.992616\n",
       "25  0.003624  0.998778  0.022789      0.993193\n",
       "26  0.003400  0.998904  0.028461      0.993286"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list=[]\n",
    "for i in range(0,len(total_df)):\n",
    "    index_list.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.insert(2,'Epoch',index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df=total_df.set_index('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.insert(4,'Epoch',index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>Epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.743577</td>\n",
       "      <td>0.491264</td>\n",
       "      <td>0.768085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.427422</td>\n",
       "      <td>0.812062</td>\n",
       "      <td>0.388238</td>\n",
       "      <td>0.833163</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.346895</td>\n",
       "      <td>0.855639</td>\n",
       "      <td>0.314708</td>\n",
       "      <td>0.870085</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.255485</td>\n",
       "      <td>0.897241</td>\n",
       "      <td>0.190771</td>\n",
       "      <td>0.926880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.180726</td>\n",
       "      <td>0.930223</td>\n",
       "      <td>0.147490</td>\n",
       "      <td>0.945870</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.136399</td>\n",
       "      <td>0.949950</td>\n",
       "      <td>0.103042</td>\n",
       "      <td>0.965298</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.106394</td>\n",
       "      <td>0.962173</td>\n",
       "      <td>0.132042</td>\n",
       "      <td>0.949132</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.087772</td>\n",
       "      <td>0.969554</td>\n",
       "      <td>0.072628</td>\n",
       "      <td>0.976201</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.070785</td>\n",
       "      <td>0.976061</td>\n",
       "      <td>0.103415</td>\n",
       "      <td>0.963818</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.058958</td>\n",
       "      <td>0.980320</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.984151</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>0.982548</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.987312</td>\n",
       "      <td>0.050081</td>\n",
       "      <td>0.983929</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.989032</td>\n",
       "      <td>0.038556</td>\n",
       "      <td>0.987623</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.028911</td>\n",
       "      <td>0.990579</td>\n",
       "      <td>0.042532</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.023191</td>\n",
       "      <td>0.992426</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.983531</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.019795</td>\n",
       "      <td>0.993454</td>\n",
       "      <td>0.070365</td>\n",
       "      <td>0.979606</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.018906</td>\n",
       "      <td>0.993541</td>\n",
       "      <td>0.045566</td>\n",
       "      <td>0.986328</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.994852</td>\n",
       "      <td>0.094505</td>\n",
       "      <td>0.974080</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.994995</td>\n",
       "      <td>0.051180</td>\n",
       "      <td>0.986188</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.996004</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>0.990789</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>0.051597</td>\n",
       "      <td>0.986847</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.996099</td>\n",
       "      <td>0.031511</td>\n",
       "      <td>0.990680</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.996542</td>\n",
       "      <td>0.040986</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>0.049138</td>\n",
       "      <td>0.986964</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.996954</td>\n",
       "      <td>0.028361</td>\n",
       "      <td>0.991722</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.996895</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>0.989231</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.009047</td>\n",
       "      <td>0.996870</td>\n",
       "      <td>0.037512</td>\n",
       "      <td>0.989363</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.996850</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.997520</td>\n",
       "      <td>0.044324</td>\n",
       "      <td>0.987690</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.057569</td>\n",
       "      <td>0.978908</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.997285</td>\n",
       "      <td>0.029673</td>\n",
       "      <td>0.991792</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.055699</td>\n",
       "      <td>0.981127</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.997571</td>\n",
       "      <td>0.031687</td>\n",
       "      <td>0.991526</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.989385</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.005651</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.991254</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.997848</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>0.990851</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.998086</td>\n",
       "      <td>0.024568</td>\n",
       "      <td>0.993664</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>0.990355</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.998092</td>\n",
       "      <td>0.055615</td>\n",
       "      <td>0.981441</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.998307</td>\n",
       "      <td>0.034108</td>\n",
       "      <td>0.991862</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.993227</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.998490</td>\n",
       "      <td>0.044173</td>\n",
       "      <td>0.989635</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.998442</td>\n",
       "      <td>0.033585</td>\n",
       "      <td>0.993107</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.997985</td>\n",
       "      <td>0.055798</td>\n",
       "      <td>0.986847</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>0.993703</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.998221</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.993603</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.998621</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.993121</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.998537</td>\n",
       "      <td>0.049858</td>\n",
       "      <td>0.985098</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.035581</td>\n",
       "      <td>0.987541</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.998644</td>\n",
       "      <td>0.046769</td>\n",
       "      <td>0.990495</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.998753</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.990756</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.030351</td>\n",
       "      <td>0.992742</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.998574</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>0.994718</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.998848</td>\n",
       "      <td>0.031905</td>\n",
       "      <td>0.991930</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.998860</td>\n",
       "      <td>0.027323</td>\n",
       "      <td>0.992302</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.992616</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.998778</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.993193</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.993286</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss  accuracy  val_loss  val_accuracy  Epoch\n",
       "Epoch                                                   \n",
       "1      0.516835  0.743577  0.491264      0.768085      1\n",
       "2      0.427422  0.812062  0.388238      0.833163      2\n",
       "3      0.346895  0.855639  0.314708      0.870085      3\n",
       "4      0.255485  0.897241  0.190771      0.926880      4\n",
       "5      0.180726  0.930223  0.147490      0.945870      5\n",
       "6      0.136399  0.949950  0.103042      0.965298      6\n",
       "7      0.106394  0.962173  0.132042      0.949132      7\n",
       "8      0.087772  0.969554  0.072628      0.976201      8\n",
       "9      0.070785  0.976061  0.103415      0.963818      9\n",
       "10     0.058958  0.980320  0.072885      0.975741     10\n",
       "11     0.047781  0.984151  0.053339      0.982548     11\n",
       "12     0.038895  0.987312  0.050081      0.983929     12\n",
       "13     0.033118  0.989032  0.038556      0.987623     13\n",
       "14     0.028911  0.990579  0.042532      0.985154     14\n",
       "15     0.023191  0.992426  0.046401      0.983531     15\n",
       "16     0.019795  0.993454  0.070365      0.979606     16\n",
       "17     0.018906  0.993541  0.045566      0.986328     17\n",
       "18     0.015225  0.994852  0.094505      0.974080     18\n",
       "19     0.014290  0.994995  0.051180      0.986188     19\n",
       "20     0.012047  0.996004  0.030915      0.990789     20\n",
       "21     0.011652  0.996094  0.051597      0.986847     21\n",
       "22     0.011799  0.996099  0.031511      0.990680     22\n",
       "23     0.010440  0.996542  0.040986      0.989436     23\n",
       "24     0.010490  0.996354  0.049138      0.986964     24\n",
       "25     0.009479  0.996954  0.028361      0.991722     25\n",
       "26     0.009459  0.996895  0.043721      0.989231     26\n",
       "27     0.009047  0.996870  0.037512      0.989363     27\n",
       "28     0.008934  0.996850  0.060384      0.982593     28\n",
       "29     0.007439  0.997520  0.044324      0.987690     29\n",
       "30     0.007758  0.997307  0.057569      0.978908     30\n",
       "31     0.007457  0.997285  0.029673      0.991792     31\n",
       "32     0.005988  0.997938  0.055699      0.981127     32\n",
       "33     0.007042  0.997571  0.031687      0.991526     33\n",
       "34     0.006263  0.997924  0.034874      0.989385     34\n",
       "35     0.005651  0.998125  0.037228      0.991254     35\n",
       "36     0.006479  0.997848  0.038642      0.990851     36\n",
       "37     0.005737  0.998086  0.024568      0.993664     37\n",
       "38     0.005101  0.998265  0.038149      0.990355     38\n",
       "39     0.005682  0.998092  0.055615      0.981441     39\n",
       "40     0.005025  0.998307  0.034108      0.991862     40\n",
       "41     0.005829  0.998221  0.023984      0.993227     41\n",
       "42     0.004662  0.998490  0.044173      0.989635     42\n",
       "43     0.004576  0.998442  0.033585      0.993107     43\n",
       "44     0.006720  0.997985  0.055798      0.986847     44\n",
       "45     0.003737  0.998750  0.023518      0.993703     45\n",
       "46     0.005260  0.998221  0.027622      0.993603     46\n",
       "47     0.004205  0.998621  0.025375      0.993121     47\n",
       "48     0.004536  0.998537  0.049858      0.985098     48\n",
       "49     0.004182  0.998560  0.035581      0.987541     49\n",
       "50     0.003930  0.998644  0.046769      0.990495     50\n",
       "51     0.003843  0.998753  0.048027      0.990756     51\n",
       "52     0.004112  0.998633  0.030351      0.992742     52\n",
       "53     0.004188  0.998574  0.022256      0.994718     53\n",
       "54     0.003368  0.998848  0.031905      0.991930     54\n",
       "55     0.003585  0.998860  0.027323      0.992302     55\n",
       "56     0.003252  0.999000  0.035009      0.992616     56\n",
       "57     0.003624  0.998778  0.022789      0.993193     57\n",
       "58     0.003400  0.998904  0.028461      0.993286     58"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x='Epoch',y='loss',data=total_df)\n",
    "sns.lineplot(x='Epoch',y='val_loss',data=total_df)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

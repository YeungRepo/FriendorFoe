{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pickle\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file_list=[]\n",
    "for root, dirs, files in os.walk('Pathogenic E. Coli Sequences/'):\n",
    "    for file in files:\n",
    "        if file.endswith('.fasta'):\n",
    "            path_file_list.append(('Pathogenic E. Coli Sequences/'+str(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_seq_list=[];\n",
    "for filename in path_file_list:\n",
    "    for seq_record in SeqIO.parse(filename,'fasta'):\n",
    "        path_seq_list.append(seq_record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpath_ecoli_fasta_file='EcoliK12_MG1655.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpath_seq_list=[];\n",
    "for seq_record in SeqIO.parse(nonpath_ecoli_fasta_file,'fasta'):\n",
    "    nonpath_seq_list.append(seq_record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpath_vector_list=[];\n",
    "for item in nonpath_seq_list:\n",
    "    vector_rep=[]\n",
    "    for letter in item:\n",
    "        if letter=='A':\n",
    "            number=0.25\n",
    "        elif letter=='T':\n",
    "            number=0.5\n",
    "        elif letter=='C':\n",
    "            number=0.75\n",
    "        elif letter=='G':\n",
    "            number=1\n",
    "        vector_rep.append(number)\n",
    "    nonpath_vector_list.append(vector_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vector_list=[];\n",
    "for item in path_seq_list:\n",
    "    vector_rep=[]\n",
    "    for letter in item:\n",
    "        if letter=='A':\n",
    "            number=0.25\n",
    "        elif letter=='T':\n",
    "            number=0.5\n",
    "        elif letter=='C':\n",
    "            number=0.75\n",
    "        elif letter=='G':\n",
    "            number=1\n",
    "        vector_rep.append(number)\n",
    "    path_vector_list.append(vector_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff=100000;\n",
    "nonpath_list=[];\n",
    "\n",
    "for item in nonpath_vector_list:\n",
    "    i=0;\n",
    "    while i+cutoff<=len(item):\n",
    "        nonpath_list.append(item[i:i+cutoff]);\n",
    "        i+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list=[];\n",
    "\n",
    "for item in path_vector_list:\n",
    "    i=0;\n",
    "    while i+cutoff<=len(item):\n",
    "        path_list.append(item[i:i+cutoff]);\n",
    "        i+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_array=np.asarray(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_nonpath_list=random.sample(nonpath_list,len(path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpath_array=np.asarray(short_nonpath_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_labelled = np.hstack((path_array, np.atleast_2d(np.ones(len(path_array))).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_path_labelled = np.hstack((nonpath_array, np.atleast_2d(np.zeros(len(nonpath_array))).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.concatenate((non_path_labelled,path_labelled),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw=data[:,-1]\n",
    "X_raw=np.delete(data,-1,axis=1)\n",
    "y_raw.reshape(len(y_raw),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_raw, y_raw.reshape(len(y_raw),1), test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('hundred_thousand_bp_trial1',X_train,y_train,X_test,y_test,'Xtrain','ytrain','Xtest','ytest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('hundred_thousand_bp_trial1.npz')\n",
    "X_train=data['arr_0']\n",
    "y_train=data['arr_1']\n",
    "X_test=data['arr_2']\n",
    "y_test=data['arr_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "y_train=y_train.reshape(y_train.shape[0])\n",
    "y_test=y_test.reshape(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# kernel size here might be changed to 2\n",
    "model.add(Conv1D(filters=20, kernel_size=20, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=20, kernel_size=18, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=20, kernel_size=16, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=20, kernel_size=10, padding='valid',activation='relu',input_shape=(np.shape(X_train)[1],1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500,activation='relu'))\n",
    "model.add(Dense(250,activation='relu'))\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#opt=keras.optimizers.Adam(learning_rate=0.0001) # This learning rate is very low\n",
    "opt=keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "#model.compile(optimizer='adam',loss='mse')\n",
    "#model.compile(optimizer='adam',loss='BinaryCrossentropy')\n",
    "\n",
    "model.compile(optimizer=opt,loss='BinaryCrossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x=X_train,y=y_train,epochs=300,validation_data=(X_test,y_test),batch_size=100) \n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('hundredthousand_bp_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df=pd.DataFrame(model.history.history)\n",
    "loss_df.to_csv('hundredthousand_bp_model_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
